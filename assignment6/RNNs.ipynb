{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "T-Npn3c68uao"
   },
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Pgji6gVz8uat"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P59NYU98GCb9",
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "61f7c266-89eb-4bc8-c291-885d73e84c66"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 16.0 MB 4.7 MB/s \n",
      "\u001B[?25h  Building wheel for bokeh (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 0.13.0 which is incompatible.\u001B[0m\n",
      "\u001B[K     |████████████████████████████████| 5.4 MB 5.3 MB/s \n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.2 which is incompatible.\n",
      "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.2 which is incompatible.\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 -qq install torch==1.11.0\n",
    "!pip3 -qq install bokeh==0.13.0\n",
    "!pip3 -qq install gensim==3.6.0\n",
    "!pip3 -qq install nltk\n",
    "!pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8sVtGHmA9aBM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-6CNKM3b4hT1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_XkoGNQUeGm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFEtWrS_4rUs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPIkKdFlHB-X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TiA2dGmgF1rW",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "bde79e47-4c8e-446e-a694-4efe99553565"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d93g_swyJA_V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QstS4NO0L97c",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "269f54f1-f600-442f-ac7a-419d87a292bd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epdW8u_YXcAv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTai8Ta0lgwL",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "da25547a-4d65-48a0-f54e-7b65dd809c34"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eChdLNGtXyP0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCjwwDs6Zq9x",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "74dae712-fe27-4ec3-a4ca-19ff7640405e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique words in train = 45441. Tags = {'DET', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NUM', 'PRT', 'ADP', 'X', 'ADJ', 'NOUN'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "URC1B2nvPGFt",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "9de9ebb0-79cb-42b4-b1fd-9bb85e66d47b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gArQwbzWWkgi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rWmSToIaeAo",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "bdf15a92-19f6-4fe6-b4e5-5f565c2ca2fa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07Ymb_MkbWsF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vjz_Rk0bbMyH",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "a69f0050-136c-4384-cf92-a5596ad1229d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWMw6QHvbaDd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8XCuxEBVbOY_",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "4626c5b0-7a9e-4fe0-9c10-fa22ecacd52e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t3xyYd__8d-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RtRbz1SwgEqc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DhsTKZalfih6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4XsRII5kW5x",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "e0acc26c-97ea-4929-f401-cda1d8be0c1b"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5I9E9P6eFYv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WVEHju54d68T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.target_size = tagset_size\n",
    "        self.lstm_layers_count = lstm_layers_count\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.fully_connected = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded_in = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embedded_in)\n",
    "        lstm_out = self.fully_connected(lstm_out)\n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_HA8zyheYGH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jbrxsZ2mehWB",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "7e823152-19a9-443f-8c59-83bff6046eab"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 92)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "def accuracy(prediction, ground_truth):\n",
    "    _, indices = torch.max(prediction, -1)\n",
    "    total = torch.sum(ground_truth > 0).item()\n",
    "    correct = torch.sum((indices == ground_truth) * (ground_truth > 0)).item()\n",
    "    return correct, total\n",
    "\n",
    "accuracy(logits, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GMUyUm1hgpe3",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "b349aad2-f469-461e-d5d9-12b0ac92b1d7"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(2.5502, grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion(logits.reshape((-1,len(tag2ind))), y_batch.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSgV3NPUpcjH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FprPQ0gllo7b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                loss = criterion(logits.reshape((-1, len(tag2ind))), y_batch.view(-1))\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                cur_correct_count, cur_sum_count = accuracy(logits, y_batch)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pqfbeh1ltEYa",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "794871c1-da7e-410b-9178-71375fc271db"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[1 / 50] Train: Loss = 0.31226, Accuracy = 71.95%: 100%|██████████| 572/572 [00:05<00:00, 98.16it/s] \n",
      "[1 / 50]   Val: Loss = 0.11070, Accuracy = 84.55%: 100%|██████████| 13/13 [00:00<00:00, 86.95it/s]\n",
      "[2 / 50] Train: Loss = 0.10060, Accuracy = 90.01%: 100%|██████████| 572/572 [00:05<00:00, 103.63it/s]\n",
      "[2 / 50]   Val: Loss = 0.08031, Accuracy = 89.17%: 100%|██████████| 13/13 [00:00<00:00, 85.39it/s]\n",
      "[3 / 50] Train: Loss = 0.06780, Accuracy = 93.22%: 100%|██████████| 572/572 [00:05<00:00, 103.30it/s]\n",
      "[3 / 50]   Val: Loss = 0.07374, Accuracy = 90.70%: 100%|██████████| 13/13 [00:00<00:00, 85.46it/s]\n",
      "[4 / 50] Train: Loss = 0.05063, Accuracy = 94.84%: 100%|██████████| 572/572 [00:06<00:00, 90.99it/s] \n",
      "[4 / 50]   Val: Loss = 0.07044, Accuracy = 91.61%: 100%|██████████| 13/13 [00:00<00:00, 80.29it/s]\n",
      "[5 / 50] Train: Loss = 0.04071, Accuracy = 95.84%: 100%|██████████| 572/572 [00:05<00:00, 103.37it/s]\n",
      "[5 / 50]   Val: Loss = 0.06985, Accuracy = 92.14%: 100%|██████████| 13/13 [00:00<00:00, 88.11it/s]\n",
      "[6 / 50] Train: Loss = 0.03303, Accuracy = 96.58%: 100%|██████████| 572/572 [00:05<00:00, 104.01it/s]\n",
      "[6 / 50]   Val: Loss = 0.06784, Accuracy = 92.41%: 100%|██████████| 13/13 [00:00<00:00, 85.03it/s]\n",
      "[7 / 50] Train: Loss = 0.02746, Accuracy = 97.14%: 100%|██████████| 572/572 [00:05<00:00, 104.79it/s]\n",
      "[7 / 50]   Val: Loss = 0.07030, Accuracy = 92.58%: 100%|██████████| 13/13 [00:00<00:00, 86.22it/s]\n",
      "[8 / 50] Train: Loss = 0.02262, Accuracy = 97.65%: 100%|██████████| 572/572 [00:05<00:00, 103.12it/s]\n",
      "[8 / 50]   Val: Loss = 0.07074, Accuracy = 92.70%: 100%|██████████| 13/13 [00:00<00:00, 84.47it/s]\n",
      "[9 / 50] Train: Loss = 0.01879, Accuracy = 98.03%: 100%|██████████| 572/572 [00:05<00:00, 102.73it/s]\n",
      "[9 / 50]   Val: Loss = 0.08043, Accuracy = 92.66%: 100%|██████████| 13/13 [00:00<00:00, 87.80it/s]\n",
      "[10 / 50] Train: Loss = 0.01568, Accuracy = 98.37%: 100%|██████████| 572/572 [00:05<00:00, 102.76it/s]\n",
      "[10 / 50]   Val: Loss = 0.08445, Accuracy = 92.50%: 100%|██████████| 13/13 [00:00<00:00, 88.60it/s]\n",
      "[11 / 50] Train: Loss = 0.01294, Accuracy = 98.67%: 100%|██████████| 572/572 [00:05<00:00, 104.11it/s]\n",
      "[11 / 50]   Val: Loss = 0.07743, Accuracy = 92.47%: 100%|██████████| 13/13 [00:00<00:00, 81.68it/s]\n",
      "[12 / 50] Train: Loss = 0.01077, Accuracy = 98.91%: 100%|██████████| 572/572 [00:05<00:00, 103.48it/s]\n",
      "[12 / 50]   Val: Loss = 0.08862, Accuracy = 92.40%: 100%|██████████| 13/13 [00:00<00:00, 85.21it/s]\n",
      "[13 / 50] Train: Loss = 0.00865, Accuracy = 99.14%: 100%|██████████| 572/572 [00:05<00:00, 102.55it/s]\n",
      "[13 / 50]   Val: Loss = 0.08387, Accuracy = 92.39%: 100%|██████████| 13/13 [00:00<00:00, 80.19it/s]\n",
      "[14 / 50] Train: Loss = 0.00713, Accuracy = 99.32%: 100%|██████████| 572/572 [00:05<00:00, 102.33it/s]\n",
      "[14 / 50]   Val: Loss = 0.08493, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 80.16it/s]\n",
      "[15 / 50] Train: Loss = 0.00585, Accuracy = 99.43%: 100%|██████████| 572/572 [00:05<00:00, 103.53it/s]\n",
      "[15 / 50]   Val: Loss = 0.09992, Accuracy = 92.18%: 100%|██████████| 13/13 [00:00<00:00, 83.52it/s]\n",
      "[16 / 50] Train: Loss = 0.00463, Accuracy = 99.57%: 100%|██████████| 572/572 [00:05<00:00, 103.24it/s]\n",
      "[16 / 50]   Val: Loss = 0.09603, Accuracy = 92.33%: 100%|██████████| 13/13 [00:00<00:00, 84.05it/s]\n",
      "[17 / 50] Train: Loss = 0.00386, Accuracy = 99.65%: 100%|██████████| 572/572 [00:05<00:00, 103.59it/s]\n",
      "[17 / 50]   Val: Loss = 0.10208, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 83.37it/s]\n",
      "[18 / 50] Train: Loss = 0.00334, Accuracy = 99.69%: 100%|██████████| 572/572 [00:05<00:00, 102.89it/s]\n",
      "[18 / 50]   Val: Loss = 0.11720, Accuracy = 92.10%: 100%|██████████| 13/13 [00:00<00:00, 85.56it/s]\n",
      "[19 / 50] Train: Loss = 0.00276, Accuracy = 99.74%: 100%|██████████| 572/572 [00:05<00:00, 102.70it/s]\n",
      "[19 / 50]   Val: Loss = 0.11430, Accuracy = 92.13%: 100%|██████████| 13/13 [00:00<00:00, 80.04it/s]\n",
      "[20 / 50] Train: Loss = 0.00243, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 103.17it/s]\n",
      "[20 / 50]   Val: Loss = 0.12187, Accuracy = 92.08%: 100%|██████████| 13/13 [00:00<00:00, 81.99it/s]\n",
      "[21 / 50] Train: Loss = 0.00215, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 102.00it/s]\n",
      "[21 / 50]   Val: Loss = 0.11864, Accuracy = 92.04%: 100%|██████████| 13/13 [00:00<00:00, 84.38it/s]\n",
      "[22 / 50] Train: Loss = 0.00196, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 102.82it/s]\n",
      "[22 / 50]   Val: Loss = 0.13255, Accuracy = 92.03%: 100%|██████████| 13/13 [00:00<00:00, 83.78it/s]\n",
      "[23 / 50] Train: Loss = 0.00206, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 103.04it/s]\n",
      "[23 / 50]   Val: Loss = 0.12654, Accuracy = 92.04%: 100%|██████████| 13/13 [00:00<00:00, 82.06it/s]\n",
      "[24 / 50] Train: Loss = 0.00192, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.79it/s]\n",
      "[24 / 50]   Val: Loss = 0.14228, Accuracy = 91.89%: 100%|██████████| 13/13 [00:00<00:00, 84.40it/s]\n",
      "[25 / 50] Train: Loss = 0.00186, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.90it/s]\n",
      "[25 / 50]   Val: Loss = 0.12915, Accuracy = 91.92%: 100%|██████████| 13/13 [00:00<00:00, 80.24it/s]\n",
      "[26 / 50] Train: Loss = 0.00178, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 101.63it/s]\n",
      "[26 / 50]   Val: Loss = 0.13473, Accuracy = 91.85%: 100%|██████████| 13/13 [00:00<00:00, 85.35it/s]\n",
      "[27 / 50] Train: Loss = 0.00159, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.76it/s]\n",
      "[27 / 50]   Val: Loss = 0.14294, Accuracy = 91.83%: 100%|██████████| 13/13 [00:00<00:00, 88.09it/s]\n",
      "[28 / 50] Train: Loss = 0.00153, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 101.76it/s]\n",
      "[28 / 50]   Val: Loss = 0.13873, Accuracy = 91.78%: 100%|██████████| 13/13 [00:00<00:00, 80.46it/s]\n",
      "[29 / 50] Train: Loss = 0.00183, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 100.90it/s]\n",
      "[29 / 50]   Val: Loss = 0.14643, Accuracy = 91.64%: 100%|██████████| 13/13 [00:00<00:00, 79.85it/s]\n",
      "[30 / 50] Train: Loss = 0.00205, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 101.17it/s]\n",
      "[30 / 50]   Val: Loss = 0.14974, Accuracy = 91.68%: 100%|██████████| 13/13 [00:00<00:00, 82.12it/s]\n",
      "[31 / 50] Train: Loss = 0.00149, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.50it/s]\n",
      "[31 / 50]   Val: Loss = 0.14936, Accuracy = 91.73%: 100%|██████████| 13/13 [00:00<00:00, 80.46it/s]\n",
      "[32 / 50] Train: Loss = 0.00140, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 101.66it/s]\n",
      "[32 / 50]   Val: Loss = 0.14824, Accuracy = 91.71%: 100%|██████████| 13/13 [00:00<00:00, 83.52it/s]\n",
      "[33 / 50] Train: Loss = 0.00139, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.90it/s]\n",
      "[33 / 50]   Val: Loss = 0.15441, Accuracy = 91.65%: 100%|██████████| 13/13 [00:00<00:00, 82.17it/s]\n",
      "[34 / 50] Train: Loss = 0.00154, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 99.59it/s] \n",
      "[34 / 50]   Val: Loss = 0.16263, Accuracy = 91.63%: 100%|██████████| 13/13 [00:00<00:00, 76.31it/s]\n",
      "[35 / 50] Train: Loss = 0.00208, Accuracy = 99.77%: 100%|██████████| 572/572 [00:05<00:00, 100.60it/s]\n",
      "[35 / 50]   Val: Loss = 0.14560, Accuracy = 91.67%: 100%|██████████| 13/13 [00:00<00:00, 82.23it/s]\n",
      "[36 / 50] Train: Loss = 0.00148, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 100.06it/s]\n",
      "[36 / 50]   Val: Loss = 0.16922, Accuracy = 91.65%: 100%|██████████| 13/13 [00:00<00:00, 84.78it/s]\n",
      "[37 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.04it/s]\n",
      "[37 / 50]   Val: Loss = 0.16686, Accuracy = 91.66%: 100%|██████████| 13/13 [00:00<00:00, 86.74it/s]\n",
      "[38 / 50] Train: Loss = 0.00128, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 98.87it/s]\n",
      "[38 / 50]   Val: Loss = 0.15259, Accuracy = 91.54%: 100%|██████████| 13/13 [00:00<00:00, 78.54it/s]\n",
      "[39 / 50] Train: Loss = 0.00131, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.75it/s]\n",
      "[39 / 50]   Val: Loss = 0.18253, Accuracy = 91.44%: 100%|██████████| 13/13 [00:00<00:00, 82.92it/s]\n",
      "[40 / 50] Train: Loss = 0.00140, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 100.52it/s]\n",
      "[40 / 50]   Val: Loss = 0.17147, Accuracy = 91.51%: 100%|██████████| 13/13 [00:00<00:00, 83.21it/s]\n",
      "[41 / 50] Train: Loss = 0.00193, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 99.92it/s]\n",
      "[41 / 50]   Val: Loss = 0.16928, Accuracy = 91.56%: 100%|██████████| 13/13 [00:00<00:00, 84.25it/s]\n",
      "[42 / 50] Train: Loss = 0.00166, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 98.67it/s]\n",
      "[42 / 50]   Val: Loss = 0.18814, Accuracy = 91.63%: 100%|██████████| 13/13 [00:00<00:00, 86.30it/s]\n",
      "[43 / 50] Train: Loss = 0.00132, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 99.29it/s]\n",
      "[43 / 50]   Val: Loss = 0.17787, Accuracy = 91.59%: 100%|██████████| 13/13 [00:00<00:00, 87.72it/s]\n",
      "[44 / 50] Train: Loss = 0.00124, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 99.87it/s]\n",
      "[44 / 50]   Val: Loss = 0.17553, Accuracy = 91.58%: 100%|██████████| 13/13 [00:00<00:00, 82.65it/s]\n",
      "[45 / 50] Train: Loss = 0.00124, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 98.84it/s] \n",
      "[45 / 50]   Val: Loss = 0.17288, Accuracy = 91.62%: 100%|██████████| 13/13 [00:00<00:00, 84.57it/s]\n",
      "[46 / 50] Train: Loss = 0.00145, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 98.52it/s]\n",
      "[46 / 50]   Val: Loss = 0.18156, Accuracy = 91.43%: 100%|██████████| 13/13 [00:00<00:00, 82.23it/s]\n",
      "[47 / 50] Train: Loss = 0.00162, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 98.54it/s]\n",
      "[47 / 50]   Val: Loss = 0.17747, Accuracy = 91.37%: 100%|██████████| 13/13 [00:00<00:00, 80.54it/s]\n",
      "[48 / 50] Train: Loss = 0.00153, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 98.64it/s] \n",
      "[48 / 50]   Val: Loss = 0.18348, Accuracy = 91.52%: 100%|██████████| 13/13 [00:00<00:00, 87.43it/s]\n",
      "[49 / 50] Train: Loss = 0.00132, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 98.90it/s]\n",
      "[49 / 50]   Val: Loss = 0.18189, Accuracy = 91.46%: 100%|██████████| 13/13 [00:00<00:00, 82.63it/s]\n",
      "[50 / 50] Train: Loss = 0.00123, Accuracy = 99.85%: 100%|██████████| 572/572 [00:05<00:00, 98.51it/s]\n",
      "[50 / 50]   Val: Loss = 0.18947, Accuracy = 91.50%: 100%|██████████| 13/13 [00:00<00:00, 87.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0qGetIhfUE5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAfV2dEOfHo5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98wr38_rw55D",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "243f9ac7-260e-462a-d547-b20cf9789c09"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "      Loss = 0.19023, Accuracy = 91.65%: 100%|██████████| 28/28 [00:00<00:00, 86.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data, batch_size):\n",
    "    loss, acc = do_epoch(model, criterion, data, batch_size)\n",
    "\n",
    "evaluate_model(model, data=(X_test, y_test), batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXUTSFaEHbDG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self.fully_connected = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding_in = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embedding_in)\n",
    "        return self.fully_connected(lstm_out)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "UTtfPNvA03ru"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = BiLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_oNAS_C1t75",
    "outputId": "705c91a6-8f85-47f3-fa49-f16c02ca8238",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[1 / 50] Train: Loss = 0.25613, Accuracy = 76.77%: 100%|██████████| 572/572 [00:06<00:00, 88.17it/s]\n",
      "[1 / 50]   Val: Loss = 0.06840, Accuracy = 89.56%: 100%|██████████| 13/13 [00:00<00:00, 61.69it/s]\n",
      "[2 / 50] Train: Loss = 0.07599, Accuracy = 92.58%: 100%|██████████| 572/572 [00:06<00:00, 88.15it/s]\n",
      "[2 / 50]   Val: Loss = 0.04331, Accuracy = 93.53%: 100%|██████████| 13/13 [00:00<00:00, 62.61it/s]\n",
      "[3 / 50] Train: Loss = 0.04924, Accuracy = 95.33%: 100%|██████████| 572/572 [00:06<00:00, 89.16it/s]\n",
      "[3 / 50]   Val: Loss = 0.03575, Accuracy = 94.90%: 100%|██████████| 13/13 [00:00<00:00, 65.75it/s]\n",
      "[4 / 50] Train: Loss = 0.03466, Accuracy = 96.72%: 100%|██████████| 572/572 [00:06<00:00, 88.73it/s]\n",
      "[4 / 50]   Val: Loss = 0.03114, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 62.41it/s]\n",
      "[5 / 50] Train: Loss = 0.02560, Accuracy = 97.63%: 100%|██████████| 572/572 [00:06<00:00, 89.67it/s]\n",
      "[5 / 50]   Val: Loss = 0.02921, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 65.20it/s]\n",
      "[6 / 50] Train: Loss = 0.01868, Accuracy = 98.29%: 100%|██████████| 572/572 [00:06<00:00, 89.54it/s]\n",
      "[6 / 50]   Val: Loss = 0.02958, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 64.64it/s]\n",
      "[7 / 50] Train: Loss = 0.01346, Accuracy = 98.81%: 100%|██████████| 572/572 [00:06<00:00, 89.40it/s]\n",
      "[7 / 50]   Val: Loss = 0.02921, Accuracy = 96.07%: 100%|██████████| 13/13 [00:00<00:00, 63.39it/s]\n",
      "[8 / 50] Train: Loss = 0.00958, Accuracy = 99.15%: 100%|██████████| 572/572 [00:06<00:00, 88.92it/s]\n",
      "[8 / 50]   Val: Loss = 0.02859, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 59.60it/s]\n",
      "[9 / 50] Train: Loss = 0.00656, Accuracy = 99.45%: 100%|██████████| 572/572 [00:06<00:00, 88.67it/s]\n",
      "[9 / 50]   Val: Loss = 0.03040, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 60.40it/s]\n",
      "[10 / 50] Train: Loss = 0.00436, Accuracy = 99.65%: 100%|██████████| 572/572 [00:06<00:00, 85.40it/s]\n",
      "[10 / 50]   Val: Loss = 0.03032, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 63.07it/s]\n",
      "[11 / 50] Train: Loss = 0.00282, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 87.41it/s]\n",
      "[11 / 50]   Val: Loss = 0.03105, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 60.95it/s]\n",
      "[12 / 50] Train: Loss = 0.00180, Accuracy = 99.89%: 100%|██████████| 572/572 [00:06<00:00, 87.22it/s]\n",
      "[12 / 50]   Val: Loss = 0.03637, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 64.67it/s]\n",
      "[13 / 50] Train: Loss = 0.00108, Accuracy = 99.95%: 100%|██████████| 572/572 [00:06<00:00, 88.41it/s]\n",
      "[13 / 50]   Val: Loss = 0.03337, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 60.97it/s]\n",
      "[14 / 50] Train: Loss = 0.00067, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 88.12it/s]\n",
      "[14 / 50]   Val: Loss = 0.03736, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 61.04it/s]\n",
      "[15 / 50] Train: Loss = 0.00040, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 88.73it/s]\n",
      "[15 / 50]   Val: Loss = 0.03860, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 63.89it/s]\n",
      "[16 / 50] Train: Loss = 0.00028, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 87.83it/s]\n",
      "[16 / 50]   Val: Loss = 0.03851, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 61.40it/s]\n",
      "[17 / 50] Train: Loss = 0.00044, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 88.04it/s]\n",
      "[17 / 50]   Val: Loss = 0.04561, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 64.39it/s]\n",
      "[18 / 50] Train: Loss = 0.00194, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 89.17it/s]\n",
      "[18 / 50]   Val: Loss = 0.03931, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 61.50it/s]\n",
      "[19 / 50] Train: Loss = 0.00042, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 88.32it/s]\n",
      "[19 / 50]   Val: Loss = 0.03948, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 60.23it/s]\n",
      "[20 / 50] Train: Loss = 0.00012, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 88.04it/s]\n",
      "[20 / 50]   Val: Loss = 0.04378, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 63.74it/s]\n",
      "[21 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.85it/s]\n",
      "[21 / 50]   Val: Loss = 0.04454, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 65.10it/s]\n",
      "[22 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.60it/s]\n",
      "[22 / 50]   Val: Loss = 0.04499, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 64.53it/s]\n",
      "[23 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.45it/s]\n",
      "[23 / 50]   Val: Loss = 0.04843, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 64.23it/s]\n",
      "[24 / 50] Train: Loss = 0.00143, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 87.48it/s]\n",
      "[24 / 50]   Val: Loss = 0.04875, Accuracy = 96.05%: 100%|██████████| 13/13 [00:00<00:00, 61.29it/s]\n",
      "[25 / 50] Train: Loss = 0.00062, Accuracy = 99.94%: 100%|██████████| 572/572 [00:06<00:00, 85.53it/s]\n",
      "[25 / 50]   Val: Loss = 0.04806, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 62.92it/s]\n",
      "[26 / 50] Train: Loss = 0.00015, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 87.31it/s]\n",
      "[26 / 50]   Val: Loss = 0.05103, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 63.93it/s]\n",
      "[27 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.53it/s]\n",
      "[27 / 50]   Val: Loss = 0.05247, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 62.40it/s]\n",
      "[28 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.08it/s]\n",
      "[28 / 50]   Val: Loss = 0.05154, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 63.45it/s]\n",
      "[29 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.66it/s]\n",
      "[29 / 50]   Val: Loss = 0.04934, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 60.06it/s]\n",
      "[30 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.95it/s]\n",
      "[30 / 50]   Val: Loss = 0.05285, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 62.48it/s]\n",
      "[31 / 50] Train: Loss = 0.00094, Accuracy = 99.90%: 100%|██████████| 572/572 [00:06<00:00, 85.44it/s]\n",
      "[31 / 50]   Val: Loss = 0.05787, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 62.76it/s]\n",
      "[32 / 50] Train: Loss = 0.00094, Accuracy = 99.90%: 100%|██████████| 572/572 [00:06<00:00, 86.50it/s]\n",
      "[32 / 50]   Val: Loss = 0.05049, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 62.09it/s]\n",
      "[33 / 50] Train: Loss = 0.00015, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 85.18it/s]\n",
      "[33 / 50]   Val: Loss = 0.05563, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 60.92it/s]\n",
      "[34 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.19it/s]\n",
      "[34 / 50]   Val: Loss = 0.04971, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 60.94it/s]\n",
      "[35 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.50it/s]\n",
      "[35 / 50]   Val: Loss = 0.05022, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 60.86it/s]\n",
      "[36 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.07it/s]\n",
      "[36 / 50]   Val: Loss = 0.05003, Accuracy = 96.35%: 100%|██████████| 13/13 [00:00<00:00, 59.39it/s]\n",
      "[37 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.77it/s]\n",
      "[37 / 50]   Val: Loss = 0.05196, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 62.47it/s]\n",
      "[38 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.00it/s]\n",
      "[38 / 50]   Val: Loss = 0.05527, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 62.58it/s]\n",
      "[39 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.15it/s]\n",
      "[39 / 50]   Val: Loss = 0.05499, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 62.49it/s]\n",
      "[40 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.26it/s]\n",
      "[40 / 50]   Val: Loss = 0.05355, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 60.38it/s]\n",
      "[41 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.47it/s]\n",
      "[41 / 50]   Val: Loss = 0.05560, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 57.20it/s]\n",
      "[42 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.21it/s]\n",
      "[42 / 50]   Val: Loss = 0.05830, Accuracy = 96.39%: 100%|██████████| 13/13 [00:00<00:00, 63.10it/s]\n",
      "[43 / 50] Train: Loss = 0.00156, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 85.72it/s]\n",
      "[43 / 50]   Val: Loss = 0.05599, Accuracy = 96.08%: 100%|██████████| 13/13 [00:00<00:00, 62.74it/s]\n",
      "[44 / 50] Train: Loss = 0.00060, Accuracy = 99.94%: 100%|██████████| 572/572 [00:06<00:00, 84.33it/s]\n",
      "[44 / 50]   Val: Loss = 0.05647, Accuracy = 96.04%: 100%|██████████| 13/13 [00:00<00:00, 62.78it/s]\n",
      "[45 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.34it/s]\n",
      "[45 / 50]   Val: Loss = 0.05934, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 51.10it/s]\n",
      "[46 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:10<00:00, 55.34it/s]\n",
      "[46 / 50]   Val: Loss = 0.05730, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 38.44it/s]\n",
      "[47 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:08<00:00, 71.26it/s]\n",
      "[47 / 50]   Val: Loss = 0.05919, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 63.79it/s]\n",
      "[48 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.64it/s]\n",
      "[48 / 50]   Val: Loss = 0.05295, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 61.93it/s]\n",
      "[49 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.30it/s]\n",
      "[49 / 50]   Val: Loss = 0.05995, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 64.26it/s]\n",
      "[50 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.34it/s]\n",
      "[50 / 50]   Val: Loss = 0.05537, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 62.94it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "evaluate_model(model, data=(X_test, y_test), batch_size=512)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1FXR5pR2IrG",
    "outputId": "da17a1cb-53fd-4f6d-e13e-b2d61b387af1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "      Loss = 0.06084, Accuracy = 96.33%: 100%|██████████| 28/28 [00:00<00:00, 64.26it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTXmYGD_ANhm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uZpY_Q1xZ18h",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "d2798e0e-c1d2-4a88-d0d1-9887b7cee910"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYogOoKlgtcf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsCstxiO03oT",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "6087dc80-7520-43aa-cf95-74bef464ff14"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcG7i-R8hbY3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "LxaRBpQd0pat",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.embedding =nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.fully_connected = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "      \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded_in = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embedded_in)\n",
    "        lstm_out = self.fully_connected(lstm_out)\n",
    "        return lstm_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EBtI6BDE-Fc7",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "0913f93e-cf7f-40b1-a4e4-4f9584f35756"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[1 / 12] Train: Loss = 0.74421, Accuracy = 78.63%: 100%|██████████| 572/572 [00:04<00:00, 126.75it/s]\n",
      "[1 / 12]   Val: Loss = 0.37151, Accuracy = 89.21%: 100%|██████████| 13/13 [00:00<00:00, 90.02it/s]\n",
      "[2 / 12] Train: Loss = 0.28670, Accuracy = 91.42%: 100%|██████████| 572/572 [00:04<00:00, 135.21it/s]\n",
      "[2 / 12]   Val: Loss = 0.26040, Accuracy = 91.96%: 100%|██████████| 13/13 [00:00<00:00, 97.22it/s]\n",
      "[3 / 12] Train: Loss = 0.21362, Accuracy = 93.32%: 100%|██████████| 572/572 [00:04<00:00, 130.17it/s]\n",
      "[3 / 12]   Val: Loss = 0.21445, Accuracy = 93.25%: 100%|██████████| 13/13 [00:00<00:00, 89.72it/s]\n",
      "[4 / 12] Train: Loss = 0.17701, Accuracy = 94.37%: 100%|██████████| 572/572 [00:04<00:00, 133.07it/s]\n",
      "[4 / 12]   Val: Loss = 0.18886, Accuracy = 93.94%: 100%|██████████| 13/13 [00:00<00:00, 97.54it/s] \n",
      "[5 / 12] Train: Loss = 0.15501, Accuracy = 94.99%: 100%|██████████| 572/572 [00:04<00:00, 130.27it/s]\n",
      "[5 / 12]   Val: Loss = 0.17263, Accuracy = 94.37%: 100%|██████████| 13/13 [00:00<00:00, 89.76it/s]\n",
      "[6 / 12] Train: Loss = 0.14052, Accuracy = 95.40%: 100%|██████████| 572/572 [00:04<00:00, 131.06it/s]\n",
      "[6 / 12]   Val: Loss = 0.16178, Accuracy = 94.70%: 100%|██████████| 13/13 [00:00<00:00, 89.43it/s]\n",
      "[7 / 12] Train: Loss = 0.13039, Accuracy = 95.65%: 100%|██████████| 572/572 [00:04<00:00, 134.24it/s]\n",
      "[7 / 12]   Val: Loss = 0.15585, Accuracy = 94.84%: 100%|██████████| 13/13 [00:00<00:00, 96.12it/s]\n",
      "[8 / 12] Train: Loss = 0.12272, Accuracy = 95.87%: 100%|██████████| 572/572 [00:04<00:00, 128.91it/s]\n",
      "[8 / 12]   Val: Loss = 0.15000, Accuracy = 94.94%: 100%|██████████| 13/13 [00:00<00:00, 91.20it/s]\n",
      "[9 / 12] Train: Loss = 0.11692, Accuracy = 96.02%: 100%|██████████| 572/572 [00:04<00:00, 117.18it/s]\n",
      "[9 / 12]   Val: Loss = 0.14743, Accuracy = 95.14%: 100%|██████████| 13/13 [00:00<00:00, 72.83it/s]\n",
      "[10 / 12] Train: Loss = 0.11225, Accuracy = 96.17%: 100%|██████████| 572/572 [00:04<00:00, 125.87it/s]\n",
      "[10 / 12]   Val: Loss = 0.14300, Accuracy = 95.20%: 100%|██████████| 13/13 [00:00<00:00, 91.36it/s]\n",
      "[11 / 12] Train: Loss = 0.10812, Accuracy = 96.28%: 100%|██████████| 572/572 [00:04<00:00, 133.44it/s]\n",
      "[11 / 12]   Val: Loss = 0.14307, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 90.27it/s]\n",
      "[12 / 12] Train: Loss = 0.10460, Accuracy = 96.39%: 100%|██████████| 572/572 [00:04<00:00, 131.23it/s]\n",
      "[12 / 12]   Val: Loss = 0.13852, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 92.02it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=12,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Ne_8f24h8kg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPUuAPGhEGVR",
    "pycharm": {
     "name": "#%%\n"
    },
    "outputId": "9872cc2a-3c5a-4c57-d61e-ded504974fda"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "      Loss = 0.13858, Accuracy = 95.32%: 100%|██████████| 28/28 [00:00<00:00, 82.65it/s]\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, data=(X_test, y_test), batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNNs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}